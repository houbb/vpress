import{_ as s}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as n,a,o as i}from"./app-NomDibRt.js";const r="/vpress/images/arch/system/9-1.jpg",p="/vpress/images/arch/system/9-2.jpg",t="/vpress/images/arch/system/9-9.jpg",l="/vpress/images/arch/system/9-10.jpg",o="/vpress/images/arch/system/9-8.jpg",c="/vpress/images/arch/system/9-7.jpg",d="/vpress/images/arch/system/9-6.jpg",h="/vpress/images/arch/system/9-5.jpg",g="/vpress/images/arch/system/9-11.jpg",b={};function v(m,e){return i(),n("div",null,e[0]||(e[0]=[a('<h1 id="grpc-网关-针对-http-2-0-长连接性能优化-提升吞吐量" tabindex="-1"><a class="header-anchor" href="#grpc-网关-针对-http-2-0-长连接性能优化-提升吞吐量"><span>gRPC 网关，针对 HTTP 2.0 长连接性能优化，提升吞吐量</span></a></h1><blockquote><p>作者：老马<br><br>公众号：老马啸西风<br><br> 博客：<a href="https://houbb.github.io/" target="_blank" rel="noopener noreferrer">https://houbb.github.io/</a><br><br> 人生理念：知行合一</p></blockquote><h2 id="业务背景" tabindex="-1"><a class="header-anchor" href="#业务背景"><span>业务背景</span></a></h2><p>最近搞在搞个网关GateWay，由于系统间请求调用是基于gRPC框架，所以网关第一职责就是能接收并转发gRPC请求，大致的系统架构如下所示:</p><p><code>简单看下即可，看不懂也没关系，后面我会对里面的核心技术点单独剖析讲解</code></p><div align="left"><img src="'+r+'" width="650px"></div><p><strong>为什么要引入网关？请求链路多了一跳，性能有损耗不说，一旦宕机就全部玩完了！</strong></p><p>但现实就是这样，不是你想怎么样，就能怎么样！</p><p><strong>有时技术方案绕一个大圈子，就是为了解决一个无法避开的因素。这个<code>因素</code>可能是多方面：</strong></p><ul><li>可能是技术上的需求，比如要做监控统计，需要在上层某个位置加个拦截层，收集数据，统一处理</li><li>可能是技术实现遇到巨大挑战，至少是当前技术团队研发实力解决不了这个难题</li><li>可能上下文会话关联，一个任务要触发多次请求，但始终要在一台机器上完成全部处理</li><li>可能是政策因素，为了数据安全，你必须走这一绕。</li></ul><p>本文引入的网关就是<strong>安全原因</strong>，由于一些公司的安全限制，外部服务无法直接访问公司内部的计算节点，需要引入<strong>一个前置网关</strong>，负责反向代理、请求路由转发、数据通信、调用监控等。</p><h2 id="问题抽象-技术选型" tabindex="-1"><a class="header-anchor" href="#问题抽象-技术选型"><span>问题抽象，技术选型</span></a></h2><p>上面的业务架构可能比较复杂，不了解业务背景同学很容易被绕晕。那么我们简化一些，抽象出一个具体要解决的问题，简化描述。</p><div align="left"><img src="'+p+`" width="650px"></div><p><strong>过程分为三步：</strong></p><p>1、client端发起gPRC调用（基于HTTP2），请求打到gRPC网关</p><p>2、网关接到请求，根据请求约定的参数标识，从Redis缓存里查询目标服务器的映射关系</p><p>3、最后，网关将请求转发给目标服务器，获取响应结果，将数据原路返回。</p><blockquote><p>gRPC必须使用 HTTP/2 传输数据，支持明文和TLS加密数据，支持流数据的交互。充分利用 HTTP/2 连接的多路复用和流式特性。</p></blockquote><p><strong>技术选型</strong></p><p>1、最早计划采用<code>Netty</code>来做，但由于<code>gRPC </code>的proto模板不是我们定义的，所以解析成本很高，另外还要读取请求Header中的数据，开发难度较大，所以这个便作为了备选方案。</p><p>2、另一种改变思路，往反向代理框架方向寻找，重新回到主流的Nginx这条线，但是nginx采用C语言开发，如果是基于常规的<code>负载均衡策略</code>转发请求，倒是没什么大的问题。但是，我们内部有依赖任务资源关系，也间接决定着要依赖外部的存储系统。</p><p>Nginx适合处理静态内容，做一个静态web服务器，但我们又看重其高性能，最后我们选型<code>OpenResty</code></p><blockquote><p>OpenResty® 是一个基于 Nginx 与 Lua 的高性能 Web 平台，其内部集成了大量精良的 Lua 库、第三方模块以及大多数的依赖项。用于方便地搭建能够处理超高并发、扩展性极高的动态 Web 应用、Web 服务和动态网关。</p></blockquote><h2 id="核心代码-show" tabindex="-1"><a class="header-anchor" href="#核心代码-show"><span>核心代码 SHOW</span></a></h2><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>http {</span></span>
<span class="line"><span>    include       mime.types;</span></span>
<span class="line"><span>    default_type  application/octet-stream;</span></span>
<span class="line"><span>    access_log  logs/access.log  main;</span></span>
<span class="line"><span>    sendfile        on;</span></span>
<span class="line"><span>    keepalive_timeout  120;</span></span>
<span class="line"><span>    client_max_body_size 3000M;</span></span>
<span class="line"><span>    server {</span></span>
<span class="line"><span>        listen   8091   http2;</span></span>
<span class="line"><span>        location / {</span></span>
<span class="line"><span>            set $target_url  &#39;&#39; ;</span></span>
<span class="line"><span>            access_by_lua_block{</span></span>
<span class="line"><span>                local headers = ngx.req.get_headers(0)</span></span>
<span class="line"><span>                local jobid= headers[&quot;jobid&quot;]</span></span>
<span class="line"><span>                local redis = require &quot;resty.redis&quot;</span></span>
<span class="line"><span>                local red = redis:new()</span></span>
<span class="line"><span>                red:set_timeouts(1000) -- 1 sec</span></span>
<span class="line"><span>                local ok, err = red:connect(&quot;156.9.1.2&quot;, 6379)</span></span>
<span class="line"><span>                local res, err = red:get(jobid)</span></span>
<span class="line"><span>                ngx.var.target_url = res</span></span>
<span class="line"><span>            }</span></span>
<span class="line"><span>            grpc_pass   grpc://$target_url;</span></span>
<span class="line"><span>        }</span></span>
<span class="line"><span>    }</span></span>
<span class="line"><span>}</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="性能压测" tabindex="-1"><a class="header-anchor" href="#性能压测"><span>性能压测</span></a></h2><h3 id="_1、client-端机器-压测期间-观察网络连接" tabindex="-1"><a class="header-anchor" href="#_1、client-端机器-压测期间-观察网络连接"><span>1、Client 端机器，压测期间，观察网络连接</span></a></h3><div align="left"><img src="`+t+'" width="650px"></div><div align="left"><img src="'+l+'" width="650px"></div><p><strong>结论：</strong></p><p>并发压测场景下，请求会转发到三台网关服务器，每台服务器处于<code>TIME_WAIT</code>状态的TCP连接并不多。可见此段连接基本能达到连接复用效果。</p><h3 id="_2、grpc网关机器-压测期间-观察网络连接情况" tabindex="-1"><a class="header-anchor" href="#_2、grpc网关机器-压测期间-观察网络连接情况"><span>2、gRPC网关机器，压测期间，观察网络连接情况</span></a></h3><div align="left"><img src="'+o+`" width="650px"></div><p>有大量的请求连接处于<code>TIME_WAIT</code>状态。按照端口号可以分为两大类：<code>6379 </code>和 <code>40928</code></p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>[root@tf-gw-64bd9f775c-qvpcx nginx]#  netstat -na | awk &#39;/^tcp/ {++S[$NF]} END {for(a in S) print a, S[a]}&#39;</span></span>
<span class="line"><span>LISTEN 2</span></span>
<span class="line"><span>ESTABLISHED 6</span></span>
<span class="line"><span>TIME_WAIT 27500</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>通过linux shell 统计命令，<code>172.16.66.46</code>服务器有27500个TCP连接处于 <code>TIME_WAIT</code></p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>[root@tf-gw-64bd9f775c-qvpcx nginx]#  netstat -na | grep 6379 |awk &#39;/^tcp/ {++S[$NF]} END {for(a in S) print a, S[a]}&#39;</span></span>
<span class="line"><span>ESTABLISHED 1</span></span>
<span class="line"><span>TIME_WAIT 13701</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>其中，连接redis（redis的访问端口 6379） 并处于 <code>TIME_WAIT</code> 状态有 13701 个连接</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>[root@tf-gw-64bd9f775c-qvpcx nginx]#  netstat -na | grep 40928 |awk &#39;/^tcp/ {++S[$NF]} END {for(a in S) print a, S[a]}&#39;</span></span>
<span class="line"><span>ESTABLISHED 2</span></span>
<span class="line"><span>TIME_WAIT 13671</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>其中，连接后端Server目标服务器 并处于 <code>TIME_WAIT</code> 状态有 13671 个连接。两者的连接数基本相等，因为每一次转发请求都要查询一次Redis。</p><div align="left"><img src="`+c+'" width="650px"></div><h3 id="结论汇总" tabindex="-1"><a class="header-anchor" href="#结论汇总"><span>结论汇总：</span></a></h3><p>1、client端发送请求到网关，目前已经维持长连接，满足要求。</p><p>2、gRPC网关连接Redis缓存服务器，目前是短连接，每次请求都去创建一个连接，性能开销太大。需要单独优化</p><p>3、gRPC网关转发请求到目标服务器，目前也是短连接，用完即废弃，完全没有发挥Http2.0的长连接优势。需要单独优化</p><h2 id="什么是-time-wait" tabindex="-1"><a class="header-anchor" href="#什么是-time-wait"><span>什么是 TIME_WAIT</span></a></h2><p>统计服务器tcp连接状态处于<code>TIME_WAIT</code>的命令脚本：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>netstat -anpt | grep TIME_WAIT | wc -l</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p><strong>我们都知道TCP是三次握手，四次挥手。那挥手具体过程是什么？</strong></p><p>1、主动关闭连接的一方，调用<strong>close()</strong>，协议层发送FIN包，主动关闭方进入<strong>FIN_WAIT_1</strong>状态</p><p>2、被动关闭的一方收到FIN包后，协议层回复ACK；然后被动关闭的一方，进入<strong>CLOSE_WAIT</strong>状态，主动关闭的一方等待对方关闭，则进入<strong>FIN_WAIT_2</strong>状态；此时，主动关闭的一方 等待 被动关闭一方的应用程序，调用<strong>close</strong>操作</p><p>3、被动关闭的一方在完成所有数据发送后，调用<strong>close()<strong>操作；此时，协议层发送FIN包给主动关闭的一方，等待对方的ACK，被动关闭的一方进入</strong>LAST_ACK</strong>状态；</p><p>4、主动关闭的一方收到FIN包，协议层回复ACK；此时，主动关闭连接的一方，进入<strong>TIME_WAIT</strong>状态；而被动关闭的一方，进入<strong>CLOSED</strong>状态</p><p>5、等待 <strong>2MSL</strong>（Maximum Segment Lifetime， 报文最大生存时间），主动关闭的一方，结束<strong>TIME_WAIT</strong>，进入<strong>CLOSED</strong>状态</p><p>2MSL到底有多长呢？这个不一定，1分钟、2分钟或者4分钟，还有的30秒。不同的发行版可能会不同。在<code>Centos 7.6.1810</code> 的3.10内核版本上是60秒。</p><p><strong>来张TCP状态机大图，一目了然：</strong></p><div align="left"><img src="'+d+`" width="650px"></div><p><strong>为什么一定要有TIME_WAIT？</strong></p><p>虽然双方都同意关闭连接了，而且握手的4个报文也都协调和发送完毕，按理可以直接到<strong>CLOSED状态</strong>。但是网络是不可靠的，发起方无法确保最后发送的ACK报文一定被对方收到，比如丢包或延迟到达，对方处于<strong>LAST_ACK状态</strong>下的SOCKET可能会因为超时未收到ACK报文，而重发FIN报文。所以<strong>TIME_WAIT状态</strong>的作用就是用来重发可能丢失的ACK报文。</p><p>简单讲，<strong>TIME_WAIT</strong>之所以等待2MSL的时长，是为了避免因为网络丢包或者网络延迟而造成的tcp传输不可靠，而这个<strong>TIME_WAIT状态</strong>则可以最大限度的提升网络传输的可靠性。</p><blockquote><p>注意：一个连接没有进入 CLOSED 状态之前，这个连接是不能被重用的！</p></blockquote><p><strong>如何优化TIME_WAIT过多的问题</strong></p><p>1、调整系统内核参数</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>net.ipv4.tcp_syncookies = 1 表示开启SYN Cookies。当出现SYN等待队列溢出时，启用cookies来处理，可防范少量SYN攻击，默认为0，表示关闭；</span></span>
<span class="line"><span>net.ipv4.tcp_tw_reuse = 1 表示开启重用。允许将 TIME-WAIT sockets重新用于新的TCP连接，默认为0，表示关闭；</span></span>
<span class="line"><span>net.ipv4.tcp_tw_recycle = 1 表示开启TCP连接中 TIME-WAIT sockets的快速回收，默认为0，表示关闭。</span></span>
<span class="line"><span>net.ipv4.tcp_fin_timeout =  修改系统默认的 TIMEOUT 时间</span></span>
<span class="line"><span>net.ipv4.tcp_max_tw_buckets = 5000 表示系统同时保持TIME_WAIT套接字的最大数量，(默认是18000). 当TIME_WAIT连接数量达到给定的值时，所有的TIME_WAIT连接会被立刻清除，并打印警告信息。但这种粗暴的清理掉所有的连接，意味着有些连接并没有成功等待2MSL，就会造成通讯异常。一般不建议调整</span></span>
<span class="line"><span>net.ipv4.tcp_timestamps = 1(默认即为1)60s内同一源ip主机的socket connect请求中的timestamp必须是递增的。也就是说服务器打开了 tcp_tw_reccycle了，就会检查时间戳，如果对方发来的包的时间戳是乱跳的或者说时间戳是滞后的，那么服务器就会丢掉不回包，现在很多公司都用LVS做负载均衡，通常是前面一台LVS，后面多台后端服务器，这其实就是NAT，当请求到达LVS后，它修改地址数据后便转发给后端服务器，但不会修改时间戳数据，对于后端服务器来说，请求的源地址就是LVS的地址，加上端口会复用，所以从后端服务器的角度看，原本不同客户端的请求经过LVS的转发，就可能会被认为是同一个连接，加之不同客户端的时间可能不一致，所以就会出现时间戳错乱的现象，于是后面的数据包就被丢弃了，具体的表现通常是是客户端明明发送的SYN，但服务端就是不响应ACK，还可以通过下面命令来确认数据包不断被丢弃的现象，所以根据情况使用</span></span>
<span class="line"><span></span></span>
<span class="line"><span>其他优化：</span></span>
<span class="line"><span>net.ipv4.ip_local_port_range = 1024 65535 ，增加可用端口范围，让系统拥有的更多的端口来建立链接，这里有个问题需要注意，对于这个设置系统就会从1025~65535这个范围内随机分配端口来用于连接，如果我们服务的使用端口比如8080刚好在这个范围之内，在升级服务期间，可能会出现8080端口被其他随机分配的链接给占用掉</span></span>
<span class="line"><span>net.ipv4.ip_local_reserved_ports = 7005,8001-8100 针对上面的问题，我们可以设置这个参数来告诉系统给我们预留哪些端口，不可以用于自动分配。</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>2、将短连接优化为长连接</p><p>短连接工作模式：连接-&gt;传输数据-&gt;关闭连接</p><p>长连接工作模式：连接-&gt;传输数据-&gt;保持连接 -&gt; 传输数据-&gt; 。。。-&gt;关闭连接</p><h2 id="访问-redis-短连接优化" tabindex="-1"><a class="header-anchor" href="#访问-redis-短连接优化"><span>访问 Redis 短连接优化</span></a></h2><p>高并发编程中，必须要使用<strong>连接池技术</strong>，把短链接改成长连接。也就是改成创建连接、收发数据、收发数据... 拆除连接，这样我们就可以减少大量创建连接、拆除连接的时间。从性能上来说肯定要比短连接好很多</p><p>在 OpenResty 中，可以设置<code>set_keepalive</code> 函数，来支持长连接。</p><p><code>set_keepalive</code> 函数有两个参数：</p><ul><li>第一个参数：连接的最大空闲时间</li><li>第二个参数：连接池大小</li></ul><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>local res, err = red:get(jobid)</span></span>
<span class="line"><span>// redis操作完后，将连接放回到连接池中</span></span>
<span class="line"><span>// 连接池大小设置成40，连接最大空闲时间设置成10秒</span></span>
<span class="line"><span>red:set_keepalive(10000, 40)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>reload nginx配置后，重新压测</p><div align="left"><img src="`+h+`" width="650px"></div><p>结论：redis的连接数基本控制在40个以内。</p><blockquote><p>其他的参数设置可以参考：<br><a href="https://github.com/openresty/lua-resty-redis#set_keepalive" target="_blank" rel="noopener noreferrer">https://github.com/openresty/lua-resty-redis#set_keepalive</a></p></blockquote><h2 id="访问-server机器短连接优化" tabindex="-1"><a class="header-anchor" href="#访问-server机器短连接优化"><span>访问 Server机器短连接优化</span></a></h2><p>nginx 提供了一个<code>upstream</code>模块，用来控制负载均衡、内容分发。提供了以下几种负载算法：</p><ul><li>轮询（默认）。每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器down掉，能自动剔除。</li><li>weight(权重)。指定轮询几率，weight和访问比率成正比，用于后端服务器性能不均的情况。</li><li>ip_hash。每个请求按访问ip的hash结果分配，这样每个访客固定访问一个后端服务器，可以解决session的问题。</li><li>fair（第三方）。按后端服务器的响应时间来分配请求，响应时间短的优先分配。</li><li>url_hash（第三方）。按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器，后端服务器为缓存时比较有效。</li></ul><p>由于 <code>upstream</code>提供了<code>keepalive</code>函数，每个工作进程的高速缓存中保留的到上游服务器的空闲保持连接的最大数量，可以保持连接复用，从而减少TCP连接频繁的创建、销毁性能开销。</p><p><strong>缺点：</strong></p><p>Nginx官方的<code>upstream</code>不支持动态修改，而我们的目标地址是动态变化，请求时根据业务规则动态实时查询路由。为了解决这个动态性问题，我们引入<code>OpenResty </code>的<code>balancer_by_lua_block</code>。</p><p>通过编写Lua脚本方式，来扩展<code>upstream</code>功能。</p><p>修改<code>nginx.conf</code>的<code>upstream</code>，动态获取路由目标的IP和Port，并完成请求的转发，核心代码如下：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span> upstream grpcservers {</span></span>
<span class="line"><span>    balancer_by_lua_block{</span></span>
<span class="line"><span>      local balancer = require &quot;ngx.balancer&quot;</span></span>
<span class="line"><span>      local host = ngx.var.target_ip</span></span>
<span class="line"><span>      local port = ngx.var.target_port</span></span>
<span class="line"><span>      local ok, err = balancer.set_current_peer(host, port)</span></span>
<span class="line"><span>      if not ok then</span></span>
<span class="line"><span>         ngx.log(ngx.ERR, &quot;failed to set the current peer: &quot;, err)</span></span>
<span class="line"><span>         return ngx.exit(500)</span></span>
<span class="line"><span>      end</span></span>
<span class="line"><span>    }</span></span>
<span class="line"><span>    keepalive 40;</span></span>
<span class="line"><span> }</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>修改配置后，重启Nginx，继续压测，观察结果：</p><div align="left"><img src="`+g+`" width="650px"></div><p>TCP连接基本都处于<code>ESTABLISHED</code>状态，优化前的<code>TIME_WAIT</code>状态几乎没有了。</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>[root@tf-gw-64bd9f775c-qvpcx nginx]#  netstat -na | awk &#39;/^tcp/ {++S[$NF]} END {for(a in S) print a, S[a]}&#39;</span></span>
<span class="line"><span>LISTEN 2</span></span>
<span class="line"><span>ESTABLISHED 86</span></span>
<span class="line"><span>TIME_WAIT 242</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="写在最后" tabindex="-1"><a class="header-anchor" href="#写在最后"><span>写在最后</span></a></h2><p>本文主要是解决gRPC的请求转发问题，构建一个网关系统，技术选型OpenResty，既保留了Nginx的高性能又兼具了OpenResty动态易扩展。然后针对编写的LUA代码，性能压测，不断调整优化，解决各个链路区间的TCP连接保证可重复使用。</p><h2 id="参考资料" tabindex="-1"><a class="header-anchor" href="#参考资料"><span>参考资料</span></a></h2><ul><li><a href="https://github.com/openresty/lua-resty-core/blob/master/lib/ngx/balancer.md" target="_blank" rel="noopener noreferrer">dynamic upstream balancers in Lua</a></li><li><a href="https://github.com/openresty/lua-resty-core/blob/master/lib/ngx/re.md#readme" target="_blank" rel="noopener noreferrer">openresty 分割字符串</a></li><li><a href="https://blog.csdn.net/zyt425916200/article/details/78113547" target="_blank" rel="noopener noreferrer">使用balancer_by_lua_block做应用层负载均衡</a></li><li><a href="https://forum.openresty.us/d/6397-tcp" target="_blank" rel="noopener noreferrer">如何编写保持长连接的tcp服务器</a></li><li><a href="https://moonbingbing.gitbooks.io/openresty-best-practices/content/web/conn_pool.html" target="_blank" rel="noopener noreferrer">redis 连接池</a></li><li><a href="https://draveness.me/whys-the-design-tcp-time-wait/" target="_blank" rel="noopener noreferrer">为什么 TCP 协议有 TIME_WAIT 状态</a></li><li><a href="https://zhuanlan.zhihu.com/p/40013724" target="_blank" rel="noopener noreferrer">系统调优你所不知道的TIME_WAIT和CLOSE_WAIT</a></li><li><a href="https://juejin.cn/post/6844903809534148622" target="_blank" rel="noopener noreferrer">用nginx做grpc反向代理，nginx到后端server不能维持长连接问题</a></li><li><a href="https://skyao.gitbooks.io/learning-nginx/content/documentation/keep_alive.html" target="_blank" rel="noopener noreferrer">支持keep alive长连接</a></li><li><a href="https://www.gushiciku.cn/pl/pS3H/zh-hk" target="_blank" rel="noopener noreferrer">nginx:反向代理到grpc server</a></li><li><a href="https://cloud.tencent.com/developer/article/1589962" target="_blank" rel="noopener noreferrer">https://cloud.tencent.com/developer/article/1589962</a></li><li><a href="https://www.cnblogs.com/rexcheny/p/11143128.html" target="_blank" rel="noopener noreferrer">https://www.cnblogs.com/rexcheny/p/11143128.html</a></li><li><a href="https://cloud.tencent.com/developer/article/1589962" target="_blank" rel="noopener noreferrer">如何优化高并发TCP链接中产生的大量的TIME_WAIT的状态</a></li><li><a href="https://www.debugger.wiki/article/html/1562139955908582" target="_blank" rel="noopener noreferrer">OpenResty 最佳实践 - TIME_WAIT</a></li><li><a href="https://bbs.huaweicloud.com/blogs/154116" target="_blank" rel="noopener noreferrer">nginx优化——包括https、keepalive等</a></li><li><a href="https://www.docs4dev.com/docs/zh/nginx/current/reference/http-ngx_http_grpc_module.html#grpc_pass" target="_blank" rel="noopener noreferrer">Nginx 中文文档</a></li></ul>`,95)]))}const k=s(b,[["render",v]]),T=JSON.parse('{"path":"/posts/interview/arch/system/gRPC%20%E7%BD%91%E5%85%B3%EF%BC%8C%E9%92%88%E5%AF%B9%20HTTP%202.0%20%E9%95%BF%E8%BF%9E%E6%8E%A5%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%EF%BC%8C%E6%8F%90%E5%8D%87%E5%90%9E%E5%90%90%E9%87%8F.html","title":"gRPC 网关，针对 HTTP 2.0 长连接性能优化，提升吞吐量","lang":"zh-CN","frontmatter":{"title":"gRPC 网关，针对 HTTP 2.0 长连接性能优化，提升吞吐量","description":"gRPC 网关，针对 HTTP 2.0 长连接性能优化，提升吞吐量 作者：老马 公众号：老马啸西风 博客：https://houbb.github.io/ 人生理念：知行合一 业务背景 最近搞在搞个网关GateWay，由于系统间请求调用是基于gRPC框架，所以网关第一职责就是能接收并转发gRPC请求，大致的系统架构如下所示: 简单看下即可，看不懂也没关...","head":[["meta",{"property":"og:url","content":"https://houbb.github.io/vpress/posts/interview/arch/system/gRPC%20%E7%BD%91%E5%85%B3%EF%BC%8C%E9%92%88%E5%AF%B9%20HTTP%202.0%20%E9%95%BF%E8%BF%9E%E6%8E%A5%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%EF%BC%8C%E6%8F%90%E5%8D%87%E5%90%9E%E5%90%90%E9%87%8F.html"}],["meta",{"property":"og:site_name","content":"老马啸西风"}],["meta",{"property":"og:title","content":"gRPC 网关，针对 HTTP 2.0 长连接性能优化，提升吞吐量"}],["meta",{"property":"og:description","content":"gRPC 网关，针对 HTTP 2.0 长连接性能优化，提升吞吐量 作者：老马 公众号：老马啸西风 博客：https://houbb.github.io/ 人生理念：知行合一 业务背景 最近搞在搞个网关GateWay，由于系统间请求调用是基于gRPC框架，所以网关第一职责就是能接收并转发gRPC请求，大致的系统架构如下所示: 简单看下即可，看不懂也没关..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-03-30T12:17:52.000Z"}],["meta",{"property":"article:modified_time","content":"2025-03-30T12:17:52.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"gRPC 网关，针对 HTTP 2.0 长连接性能优化，提升吞吐量\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-03-30T12:17:52.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"老马啸西风\\",\\"url\\":\\"https://houbb.github.io\\"}]}"]]},"git":{"createdTime":1743327520000,"updatedTime":1743337072000,"contributors":[{"name":"houbb","username":"houbb","email":"houbinbin.echo@gmail.com","commits":2,"url":"https://github.com/houbb"}]},"readingTime":{"minutes":12.94,"words":3882},"filePathRelative":"posts/interview/arch/system/gRPC 网关，针对 HTTP 2.0 长连接性能优化，提升吞吐量.md","localizedDate":"2025年3月30日","excerpt":"\\n<blockquote>\\n<p>作者：老马<br>\\n<br>公众号：老马啸西风<br>\\n<br> 博客：<a href=\\"https://houbb.github.io/\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">https://houbb.github.io/</a><br>\\n<br> 人生理念：知行合一</p>\\n</blockquote>\\n<h2>业务背景</h2>\\n<p>最近搞在搞个网关GateWay，由于系统间请求调用是基于gRPC框架，所以网关第一职责就是能接收并转发gRPC请求，大致的系统架构如下所示:</p>\\n<p><code>简单看下即可，看不懂也没关系，后面我会对里面的核心技术点单独剖析讲解</code></p>","autoDesc":true}');export{k as comp,T as data};
